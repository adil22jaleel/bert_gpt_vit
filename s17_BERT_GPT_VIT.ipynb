{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a72e33b1b6442daba3eb9c59477aeea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ea5c43c605c49d5b7e463673e78dedc",
              "IPY_MODEL_d101529ab25345bf816924d8feaad369",
              "IPY_MODEL_c7a59caf15b44fb3ba8c499140ff37e4"
            ],
            "layout": "IPY_MODEL_6f7d130a0a894d51a82e37d9456e030d"
          }
        },
        "6ea5c43c605c49d5b7e463673e78dedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2ebaa76c6d84b79929c51db29736680",
            "placeholder": "​",
            "style": "IPY_MODEL_320e4ea320d14bd8a7218f2d960cdd78",
            "value": "100%"
          }
        },
        "d101529ab25345bf816924d8feaad369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa7e73c7aa1e40e08aafa879ae760755",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73921466763f4f3c89bd241ba2614700",
            "value": 50
          }
        },
        "c7a59caf15b44fb3ba8c499140ff37e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0df377f22da14914b24cea651f670bc4",
            "placeholder": "​",
            "style": "IPY_MODEL_30349fd000be4ef2a972b2e1ec59ae0e",
            "value": " 50/50 [02:39&lt;00:00,  3.17s/it]"
          }
        },
        "6f7d130a0a894d51a82e37d9456e030d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2ebaa76c6d84b79929c51db29736680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320e4ea320d14bd8a7218f2d960cdd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa7e73c7aa1e40e08aafa879ae760755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73921466763f4f3c89bd241ba2614700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0df377f22da14914b24cea651f670bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30349fd000be4ef2a972b2e1ec59ae0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsfzQJD4lB2z",
        "outputId": "d5a5ce00-ca62-4ba7-9284-dc17fd1dc1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/adil22jaleel/bert_gpt_vit.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbpqQWKUlIjZ",
        "outputId": "4a82d7fd-3e6e-417a-ee67-f7ec813963bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert_gpt_vit'...\n",
            "remote: Enumerating objects: 372, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 372 (delta 5), reused 85 (delta 2), pack-reused 280\u001b[K\n",
            "Receiving objects: 100% (372/372), 17.08 MiB | 14.24 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_gpt_vit.transformers import *"
      ],
      "metadata": {
        "id": "di-hmXl8l_Tk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers_combined(\"BERT\",500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_9oRbBfmJmH",
        "outputId": "8d61b10e-5129-448e-bbe7-263f592709b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITERATION: 1  | Loss 10.15  | ΔW: 1.155\n",
            "ITERATION: 2  | Loss 10.06  | ΔW: 1.085\n",
            "ITERATION: 3  | Loss 9.94  | ΔW: 1.021\n",
            "ITERATION: 4  | Loss 9.88  | ΔW: 0.938\n",
            "ITERATION: 5  | Loss 9.81  | ΔW: 0.851\n",
            "ITERATION: 6  | Loss 9.72  | ΔW: 0.788\n",
            "ITERATION: 7  | Loss 9.69  | ΔW: 0.717\n",
            "ITERATION: 8  | Loss 9.66  | ΔW: 0.655\n",
            "ITERATION: 9  | Loss 9.6  | ΔW: 0.618\n",
            "ITERATION: 10  | Loss 9.53  | ΔW: 0.578\n",
            "ITERATION: 11  | Loss 9.48  | ΔW: 0.548\n",
            "ITERATION: 12  | Loss 9.47  | ΔW: 0.515\n",
            "ITERATION: 13  | Loss 9.49  | ΔW: 0.487\n",
            "ITERATION: 14  | Loss 9.42  | ΔW: 0.476\n",
            "ITERATION: 15  | Loss 9.39  | ΔW: 0.457\n",
            "ITERATION: 16  | Loss 9.42  | ΔW: 0.432\n",
            "ITERATION: 17  | Loss 9.37  | ΔW: 0.421\n",
            "ITERATION: 18  | Loss 9.34  | ΔW: 0.388\n",
            "ITERATION: 19  | Loss 9.34  | ΔW: 0.408\n",
            "ITERATION: 20  | Loss 9.36  | ΔW: 0.367\n",
            "ITERATION: 21  | Loss 9.26  | ΔW: 0.367\n",
            "ITERATION: 22  | Loss 9.24  | ΔW: 0.37\n",
            "ITERATION: 23  | Loss 9.29  | ΔW: 0.353\n",
            "ITERATION: 24  | Loss 9.24  | ΔW: 0.346\n",
            "ITERATION: 25  | Loss 9.2  | ΔW: 0.328\n",
            "ITERATION: 26  | Loss 9.2  | ΔW: 0.323\n",
            "ITERATION: 27  | Loss 9.2  | ΔW: 0.307\n",
            "ITERATION: 28  | Loss 9.17  | ΔW: 0.293\n",
            "ITERATION: 29  | Loss 9.2  | ΔW: 0.302\n",
            "ITERATION: 30  | Loss 9.15  | ΔW: 0.286\n",
            "ITERATION: 31  | Loss 9.16  | ΔW: 0.279\n",
            "ITERATION: 32  | Loss 9.07  | ΔW: 0.28\n",
            "ITERATION: 33  | Loss 9.16  | ΔW: 0.284\n",
            "ITERATION: 34  | Loss 9.11  | ΔW: 0.263\n",
            "ITERATION: 35  | Loss 9.07  | ΔW: 0.264\n",
            "ITERATION: 36  | Loss 9.04  | ΔW: 0.256\n",
            "ITERATION: 37  | Loss 9.0  | ΔW: 0.256\n",
            "ITERATION: 38  | Loss 8.99  | ΔW: 0.249\n",
            "ITERATION: 39  | Loss 9.03  | ΔW: 0.262\n",
            "ITERATION: 40  | Loss 9.01  | ΔW: 0.245\n",
            "ITERATION: 41  | Loss 9.02  | ΔW: 0.246\n",
            "ITERATION: 42  | Loss 8.95  | ΔW: 0.236\n",
            "ITERATION: 43  | Loss 8.95  | ΔW: 0.24\n",
            "ITERATION: 44  | Loss 8.9  | ΔW: 0.226\n",
            "ITERATION: 45  | Loss 8.91  | ΔW: 0.229\n",
            "ITERATION: 46  | Loss 8.86  | ΔW: 0.23\n",
            "ITERATION: 47  | Loss 8.88  | ΔW: 0.221\n",
            "ITERATION: 48  | Loss 8.83  | ΔW: 0.223\n",
            "ITERATION: 49  | Loss 8.91  | ΔW: 0.223\n",
            "ITERATION: 50  | Loss 8.85  | ΔW: 0.218\n",
            "ITERATION: 51  | Loss 8.83  | ΔW: 0.214\n",
            "ITERATION: 52  | Loss 8.79  | ΔW: 0.225\n",
            "ITERATION: 53  | Loss 8.81  | ΔW: 0.209\n",
            "ITERATION: 54  | Loss 8.77  | ΔW: 0.211\n",
            "ITERATION: 55  | Loss 8.74  | ΔW: 0.213\n",
            "ITERATION: 56  | Loss 8.75  | ΔW: 0.206\n",
            "ITERATION: 57  | Loss 8.72  | ΔW: 0.204\n",
            "ITERATION: 58  | Loss 8.71  | ΔW: 0.212\n",
            "ITERATION: 59  | Loss 8.68  | ΔW: 0.21\n",
            "ITERATION: 60  | Loss 8.65  | ΔW: 0.203\n",
            "ITERATION: 61  | Loss 8.67  | ΔW: 0.199\n",
            "ITERATION: 62  | Loss 8.64  | ΔW: 0.195\n",
            "ITERATION: 63  | Loss 8.6  | ΔW: 0.201\n",
            "ITERATION: 64  | Loss 8.64  | ΔW: 0.195\n",
            "ITERATION: 65  | Loss 8.61  | ΔW: 0.192\n",
            "ITERATION: 66  | Loss 8.6  | ΔW: 0.201\n",
            "ITERATION: 67  | Loss 8.63  | ΔW: 0.198\n",
            "ITERATION: 68  | Loss 8.53  | ΔW: 0.199\n",
            "ITERATION: 69  | Loss 8.51  | ΔW: 0.194\n",
            "ITERATION: 70  | Loss 8.51  | ΔW: 0.194\n",
            "ITERATION: 71  | Loss 8.51  | ΔW: 0.197\n",
            "ITERATION: 72  | Loss 8.44  | ΔW: 0.196\n",
            "ITERATION: 73  | Loss 8.39  | ΔW: 0.194\n",
            "ITERATION: 74  | Loss 8.49  | ΔW: 0.184\n",
            "ITERATION: 75  | Loss 8.48  | ΔW: 0.185\n",
            "ITERATION: 76  | Loss 8.46  | ΔW: 0.196\n",
            "ITERATION: 77  | Loss 8.41  | ΔW: 0.186\n",
            "ITERATION: 78  | Loss 8.41  | ΔW: 0.186\n",
            "ITERATION: 79  | Loss 8.41  | ΔW: 0.185\n",
            "ITERATION: 80  | Loss 8.37  | ΔW: 0.181\n",
            "ITERATION: 81  | Loss 8.37  | ΔW: 0.178\n",
            "ITERATION: 82  | Loss 8.34  | ΔW: 0.177\n",
            "ITERATION: 83  | Loss 8.32  | ΔW: 0.172\n",
            "ITERATION: 84  | Loss 8.31  | ΔW: 0.184\n",
            "ITERATION: 85  | Loss 8.29  | ΔW: 0.175\n",
            "ITERATION: 86  | Loss 8.28  | ΔW: 0.179\n",
            "ITERATION: 87  | Loss 8.3  | ΔW: 0.173\n",
            "ITERATION: 88  | Loss 8.25  | ΔW: 0.176\n",
            "ITERATION: 89  | Loss 8.24  | ΔW: 0.177\n",
            "ITERATION: 90  | Loss 8.21  | ΔW: 0.176\n",
            "ITERATION: 91  | Loss 8.2  | ΔW: 0.174\n",
            "ITERATION: 92  | Loss 8.16  | ΔW: 0.177\n",
            "ITERATION: 93  | Loss 8.17  | ΔW: 0.181\n",
            "ITERATION: 94  | Loss 8.11  | ΔW: 0.173\n",
            "ITERATION: 95  | Loss 8.16  | ΔW: 0.167\n",
            "ITERATION: 96  | Loss 8.14  | ΔW: 0.162\n",
            "ITERATION: 97  | Loss 8.16  | ΔW: 0.168\n",
            "ITERATION: 98  | Loss 8.07  | ΔW: 0.164\n",
            "ITERATION: 99  | Loss 8.08  | ΔW: 0.169\n",
            "ITERATION: 100  | Loss 8.08  | ΔW: 0.165\n",
            "ITERATION: 101  | Loss 8.06  | ΔW: 0.165\n",
            "ITERATION: 102  | Loss 8.06  | ΔW: 0.168\n",
            "ITERATION: 103  | Loss 8.02  | ΔW: 0.169\n",
            "ITERATION: 104  | Loss 8.03  | ΔW: 0.164\n",
            "ITERATION: 105  | Loss 7.95  | ΔW: 0.163\n",
            "ITERATION: 106  | Loss 8.03  | ΔW: 0.159\n",
            "ITERATION: 107  | Loss 7.94  | ΔW: 0.161\n",
            "ITERATION: 108  | Loss 7.98  | ΔW: 0.163\n",
            "ITERATION: 109  | Loss 7.92  | ΔW: 0.167\n",
            "ITERATION: 110  | Loss 7.97  | ΔW: 0.169\n",
            "ITERATION: 111  | Loss 7.91  | ΔW: 0.163\n",
            "ITERATION: 112  | Loss 7.94  | ΔW: 0.161\n",
            "ITERATION: 113  | Loss 7.92  | ΔW: 0.155\n",
            "ITERATION: 114  | Loss 7.91  | ΔW: 0.154\n",
            "ITERATION: 115  | Loss 7.89  | ΔW: 0.162\n",
            "ITERATION: 116  | Loss 7.83  | ΔW: 0.159\n",
            "ITERATION: 117  | Loss 7.86  | ΔW: 0.154\n",
            "ITERATION: 118  | Loss 7.86  | ΔW: 0.149\n",
            "ITERATION: 119  | Loss 7.82  | ΔW: 0.153\n",
            "ITERATION: 120  | Loss 7.78  | ΔW: 0.154\n",
            "ITERATION: 121  | Loss 7.82  | ΔW: 0.153\n",
            "ITERATION: 122  | Loss 7.81  | ΔW: 0.152\n",
            "ITERATION: 123  | Loss 7.8  | ΔW: 0.154\n",
            "ITERATION: 124  | Loss 7.71  | ΔW: 0.158\n",
            "ITERATION: 125  | Loss 7.74  | ΔW: 0.149\n",
            "ITERATION: 126  | Loss 7.67  | ΔW: 0.15\n",
            "ITERATION: 127  | Loss 7.73  | ΔW: 0.145\n",
            "ITERATION: 128  | Loss 7.66  | ΔW: 0.151\n",
            "ITERATION: 129  | Loss 7.65  | ΔW: 0.153\n",
            "ITERATION: 130  | Loss 7.67  | ΔW: 0.149\n",
            "ITERATION: 131  | Loss 7.65  | ΔW: 0.152\n",
            "ITERATION: 132  | Loss 7.56  | ΔW: 0.152\n",
            "ITERATION: 133  | Loss 7.6  | ΔW: 0.145\n",
            "ITERATION: 134  | Loss 7.65  | ΔW: 0.154\n",
            "ITERATION: 135  | Loss 7.63  | ΔW: 0.155\n",
            "ITERATION: 136  | Loss 7.56  | ΔW: 0.145\n",
            "ITERATION: 137  | Loss 7.6  | ΔW: 0.148\n",
            "ITERATION: 138  | Loss 7.55  | ΔW: 0.154\n",
            "ITERATION: 139  | Loss 7.57  | ΔW: 0.148\n",
            "ITERATION: 140  | Loss 7.53  | ΔW: 0.147\n",
            "ITERATION: 141  | Loss 7.53  | ΔW: 0.148\n",
            "ITERATION: 142  | Loss 7.53  | ΔW: 0.146\n",
            "ITERATION: 143  | Loss 7.52  | ΔW: 0.142\n",
            "ITERATION: 144  | Loss 7.52  | ΔW: 0.156\n",
            "ITERATION: 145  | Loss 7.43  | ΔW: 0.143\n",
            "ITERATION: 146  | Loss 7.5  | ΔW: 0.142\n",
            "ITERATION: 147  | Loss 7.49  | ΔW: 0.145\n",
            "ITERATION: 148  | Loss 7.46  | ΔW: 0.142\n",
            "ITERATION: 149  | Loss 7.47  | ΔW: 0.144\n",
            "ITERATION: 150  | Loss 7.41  | ΔW: 0.153\n",
            "ITERATION: 151  | Loss 7.42  | ΔW: 0.151\n",
            "ITERATION: 152  | Loss 7.4  | ΔW: 0.141\n",
            "ITERATION: 153  | Loss 7.42  | ΔW: 0.143\n",
            "ITERATION: 154  | Loss 7.36  | ΔW: 0.143\n",
            "ITERATION: 155  | Loss 7.43  | ΔW: 0.15\n",
            "ITERATION: 156  | Loss 7.42  | ΔW: 0.144\n",
            "ITERATION: 157  | Loss 7.35  | ΔW: 0.142\n",
            "ITERATION: 158  | Loss 7.39  | ΔW: 0.145\n",
            "ITERATION: 159  | Loss 7.39  | ΔW: 0.137\n",
            "ITERATION: 160  | Loss 7.32  | ΔW: 0.147\n",
            "ITERATION: 161  | Loss 7.34  | ΔW: 0.14\n",
            "ITERATION: 162  | Loss 7.27  | ΔW: 0.147\n",
            "ITERATION: 163  | Loss 7.25  | ΔW: 0.149\n",
            "ITERATION: 164  | Loss 7.31  | ΔW: 0.14\n",
            "ITERATION: 165  | Loss 7.25  | ΔW: 0.142\n",
            "ITERATION: 166  | Loss 7.22  | ΔW: 0.143\n",
            "ITERATION: 167  | Loss 7.3  | ΔW: 0.137\n",
            "ITERATION: 168  | Loss 7.26  | ΔW: 0.146\n",
            "ITERATION: 169  | Loss 7.24  | ΔW: 0.154\n",
            "ITERATION: 170  | Loss 7.22  | ΔW: 0.141\n",
            "ITERATION: 171  | Loss 7.28  | ΔW: 0.137\n",
            "ITERATION: 172  | Loss 7.21  | ΔW: 0.142\n",
            "ITERATION: 173  | Loss 7.21  | ΔW: 0.139\n",
            "ITERATION: 174  | Loss 7.2  | ΔW: 0.134\n",
            "ITERATION: 175  | Loss 7.18  | ΔW: 0.136\n",
            "ITERATION: 176  | Loss 7.19  | ΔW: 0.14\n",
            "ITERATION: 177  | Loss 7.18  | ΔW: 0.138\n",
            "ITERATION: 178  | Loss 7.14  | ΔW: 0.135\n",
            "ITERATION: 179  | Loss 7.2  | ΔW: 0.135\n",
            "ITERATION: 180  | Loss 7.17  | ΔW: 0.145\n",
            "ITERATION: 181  | Loss 7.12  | ΔW: 0.138\n",
            "ITERATION: 182  | Loss 7.09  | ΔW: 0.148\n",
            "ITERATION: 183  | Loss 7.07  | ΔW: 0.134\n",
            "ITERATION: 184  | Loss 7.11  | ΔW: 0.136\n",
            "ITERATION: 185  | Loss 7.07  | ΔW: 0.135\n",
            "ITERATION: 186  | Loss 7.04  | ΔW: 0.134\n",
            "ITERATION: 187  | Loss 7.12  | ΔW: 0.134\n",
            "ITERATION: 188  | Loss 7.03  | ΔW: 0.13\n",
            "ITERATION: 189  | Loss 7.08  | ΔW: 0.134\n",
            "ITERATION: 190  | Loss 7.02  | ΔW: 0.131\n",
            "ITERATION: 191  | Loss 7.13  | ΔW: 0.132\n",
            "ITERATION: 192  | Loss 6.98  | ΔW: 0.144\n",
            "ITERATION: 193  | Loss 6.99  | ΔW: 0.137\n",
            "ITERATION: 194  | Loss 7.1  | ΔW: 0.136\n",
            "ITERATION: 195  | Loss 6.98  | ΔW: 0.135\n",
            "ITERATION: 196  | Loss 6.97  | ΔW: 0.134\n",
            "ITERATION: 197  | Loss 7.06  | ΔW: 0.129\n",
            "ITERATION: 198  | Loss 6.95  | ΔW: 0.131\n",
            "ITERATION: 199  | Loss 6.98  | ΔW: 0.137\n",
            "ITERATION: 200  | Loss 6.93  | ΔW: 0.134\n",
            "ITERATION: 201  | Loss 6.94  | ΔW: 0.145\n",
            "ITERATION: 202  | Loss 6.9  | ΔW: 0.138\n",
            "ITERATION: 203  | Loss 6.92  | ΔW: 0.14\n",
            "ITERATION: 204  | Loss 6.95  | ΔW: 0.135\n",
            "ITERATION: 205  | Loss 6.98  | ΔW: 0.136\n",
            "ITERATION: 206  | Loss 6.89  | ΔW: 0.136\n",
            "ITERATION: 207  | Loss 6.92  | ΔW: 0.14\n",
            "ITERATION: 208  | Loss 6.82  | ΔW: 0.143\n",
            "ITERATION: 209  | Loss 6.85  | ΔW: 0.135\n",
            "ITERATION: 210  | Loss 6.87  | ΔW: 0.138\n",
            "ITERATION: 211  | Loss 6.87  | ΔW: 0.145\n",
            "ITERATION: 212  | Loss 6.95  | ΔW: 0.142\n",
            "ITERATION: 213  | Loss 6.89  | ΔW: 0.137\n",
            "ITERATION: 214  | Loss 6.86  | ΔW: 0.131\n",
            "ITERATION: 215  | Loss 6.82  | ΔW: 0.141\n",
            "ITERATION: 216  | Loss 6.84  | ΔW: 0.139\n",
            "ITERATION: 217  | Loss 6.87  | ΔW: 0.147\n",
            "ITERATION: 218  | Loss 6.83  | ΔW: 0.132\n",
            "ITERATION: 219  | Loss 6.86  | ΔW: 0.132\n",
            "ITERATION: 220  | Loss 6.92  | ΔW: 0.142\n",
            "ITERATION: 221  | Loss 6.8  | ΔW: 0.143\n",
            "ITERATION: 222  | Loss 6.76  | ΔW: 0.142\n",
            "ITERATION: 223  | Loss 6.84  | ΔW: 0.141\n",
            "ITERATION: 224  | Loss 6.77  | ΔW: 0.131\n",
            "ITERATION: 225  | Loss 6.85  | ΔW: 0.163\n",
            "ITERATION: 226  | Loss 6.78  | ΔW: 0.144\n",
            "ITERATION: 227  | Loss 6.8  | ΔW: 0.139\n",
            "ITERATION: 228  | Loss 6.82  | ΔW: 0.137\n",
            "ITERATION: 229  | Loss 6.85  | ΔW: 0.132\n",
            "ITERATION: 230  | Loss 6.78  | ΔW: 0.143\n",
            "ITERATION: 231  | Loss 6.66  | ΔW: 0.14\n",
            "ITERATION: 232  | Loss 6.79  | ΔW: 0.138\n",
            "ITERATION: 233  | Loss 6.76  | ΔW: 0.146\n",
            "ITERATION: 234  | Loss 6.79  | ΔW: 0.136\n",
            "ITERATION: 235  | Loss 6.77  | ΔW: 0.138\n",
            "ITERATION: 236  | Loss 6.7  | ΔW: 0.143\n",
            "ITERATION: 237  | Loss 6.76  | ΔW: 0.135\n",
            "ITERATION: 238  | Loss 6.75  | ΔW: 0.135\n",
            "ITERATION: 239  | Loss 6.73  | ΔW: 0.14\n",
            "ITERATION: 240  | Loss 6.79  | ΔW: 0.142\n",
            "ITERATION: 241  | Loss 6.65  | ΔW: 0.139\n",
            "ITERATION: 242  | Loss 6.75  | ΔW: 0.139\n",
            "ITERATION: 243  | Loss 6.76  | ΔW: 0.137\n",
            "ITERATION: 244  | Loss 6.66  | ΔW: 0.138\n",
            "ITERATION: 245  | Loss 6.73  | ΔW: 0.144\n",
            "ITERATION: 246  | Loss 6.74  | ΔW: 0.14\n",
            "ITERATION: 247  | Loss 6.66  | ΔW: 0.141\n",
            "ITERATION: 248  | Loss 6.76  | ΔW: 0.136\n",
            "ITERATION: 249  | Loss 6.68  | ΔW: 0.149\n",
            "ITERATION: 250  | Loss 6.75  | ΔW: 0.134\n",
            "ITERATION: 251  | Loss 6.7  | ΔW: 0.14\n",
            "ITERATION: 252  | Loss 6.73  | ΔW: 0.132\n",
            "ITERATION: 253  | Loss 6.72  | ΔW: 0.14\n",
            "ITERATION: 254  | Loss 6.58  | ΔW: 0.144\n",
            "ITERATION: 255  | Loss 6.71  | ΔW: 0.145\n",
            "ITERATION: 256  | Loss 6.63  | ΔW: 0.132\n",
            "ITERATION: 257  | Loss 6.66  | ΔW: 0.139\n",
            "ITERATION: 258  | Loss 6.57  | ΔW: 0.136\n",
            "ITERATION: 259  | Loss 6.67  | ΔW: 0.142\n",
            "ITERATION: 260  | Loss 6.65  | ΔW: 0.14\n",
            "ITERATION: 261  | Loss 6.58  | ΔW: 0.152\n",
            "ITERATION: 262  | Loss 6.61  | ΔW: 0.141\n",
            "ITERATION: 263  | Loss 6.58  | ΔW: 0.14\n",
            "ITERATION: 264  | Loss 6.67  | ΔW: 0.139\n",
            "ITERATION: 265  | Loss 6.62  | ΔW: 0.137\n",
            "ITERATION: 266  | Loss 6.6  | ΔW: 0.14\n",
            "ITERATION: 267  | Loss 6.57  | ΔW: 0.147\n",
            "ITERATION: 268  | Loss 6.56  | ΔW: 0.148\n",
            "ITERATION: 269  | Loss 6.54  | ΔW: 0.141\n",
            "ITERATION: 270  | Loss 6.54  | ΔW: 0.154\n",
            "ITERATION: 271  | Loss 6.54  | ΔW: 0.154\n",
            "ITERATION: 272  | Loss 6.54  | ΔW: 0.143\n",
            "ITERATION: 273  | Loss 6.57  | ΔW: 0.141\n",
            "ITERATION: 274  | Loss 6.53  | ΔW: 0.142\n",
            "ITERATION: 275  | Loss 6.56  | ΔW: 0.15\n",
            "ITERATION: 276  | Loss 6.56  | ΔW: 0.148\n",
            "ITERATION: 277  | Loss 6.57  | ΔW: 0.152\n",
            "ITERATION: 278  | Loss 6.57  | ΔW: 0.141\n",
            "ITERATION: 279  | Loss 6.57  | ΔW: 0.142\n",
            "ITERATION: 280  | Loss 6.6  | ΔW: 0.147\n",
            "ITERATION: 281  | Loss 6.52  | ΔW: 0.148\n",
            "ITERATION: 282  | Loss 6.57  | ΔW: 0.148\n",
            "ITERATION: 283  | Loss 6.52  | ΔW: 0.146\n",
            "ITERATION: 284  | Loss 6.55  | ΔW: 0.151\n",
            "ITERATION: 285  | Loss 6.52  | ΔW: 0.173\n",
            "ITERATION: 286  | Loss 6.55  | ΔW: 0.157\n",
            "ITERATION: 287  | Loss 6.49  | ΔW: 0.144\n",
            "ITERATION: 288  | Loss 6.6  | ΔW: 0.145\n",
            "ITERATION: 289  | Loss 6.46  | ΔW: 0.149\n",
            "ITERATION: 290  | Loss 6.51  | ΔW: 0.147\n",
            "ITERATION: 291  | Loss 6.46  | ΔW: 0.147\n",
            "ITERATION: 292  | Loss 6.53  | ΔW: 0.166\n",
            "ITERATION: 293  | Loss 6.52  | ΔW: 0.165\n",
            "ITERATION: 294  | Loss 6.55  | ΔW: 0.147\n",
            "ITERATION: 295  | Loss 6.52  | ΔW: 0.149\n",
            "ITERATION: 296  | Loss 6.61  | ΔW: 0.148\n",
            "ITERATION: 297  | Loss 6.56  | ΔW: 0.174\n",
            "ITERATION: 298  | Loss 6.6  | ΔW: 0.162\n",
            "ITERATION: 299  | Loss 6.48  | ΔW: 0.152\n",
            "ITERATION: 300  | Loss 6.48  | ΔW: 0.152\n",
            "ITERATION: 301  | Loss 6.52  | ΔW: 0.154\n",
            "ITERATION: 302  | Loss 6.54  | ΔW: 0.153\n",
            "ITERATION: 303  | Loss 6.53  | ΔW: 0.159\n",
            "ITERATION: 304  | Loss 6.5  | ΔW: 0.159\n",
            "ITERATION: 305  | Loss 6.47  | ΔW: 0.167\n",
            "ITERATION: 306  | Loss 6.52  | ΔW: 0.149\n",
            "ITERATION: 307  | Loss 6.49  | ΔW: 0.156\n",
            "ITERATION: 308  | Loss 6.45  | ΔW: 0.16\n",
            "ITERATION: 309  | Loss 6.45  | ΔW: 0.158\n",
            "ITERATION: 310  | Loss 6.5  | ΔW: 0.156\n",
            "ITERATION: 311  | Loss 6.52  | ΔW: 0.153\n",
            "ITERATION: 312  | Loss 6.53  | ΔW: 0.176\n",
            "ITERATION: 313  | Loss 6.4  | ΔW: 0.163\n",
            "ITERATION: 314  | Loss 6.47  | ΔW: 0.158\n",
            "ITERATION: 315  | Loss 6.42  | ΔW: 0.157\n",
            "ITERATION: 316  | Loss 6.35  | ΔW: 0.157\n",
            "ITERATION: 317  | Loss 6.47  | ΔW: 0.158\n",
            "ITERATION: 318  | Loss 6.43  | ΔW: 0.174\n",
            "ITERATION: 319  | Loss 6.49  | ΔW: 0.16\n",
            "ITERATION: 320  | Loss 6.42  | ΔW: 0.155\n",
            "ITERATION: 321  | Loss 6.46  | ΔW: 0.177\n",
            "ITERATION: 322  | Loss 6.41  | ΔW: 0.168\n",
            "ITERATION: 323  | Loss 6.43  | ΔW: 0.172\n",
            "ITERATION: 324  | Loss 6.51  | ΔW: 0.163\n",
            "ITERATION: 325  | Loss 6.43  | ΔW: 0.157\n",
            "ITERATION: 326  | Loss 6.46  | ΔW: 0.164\n",
            "ITERATION: 327  | Loss 6.49  | ΔW: 0.159\n",
            "ITERATION: 328  | Loss 6.5  | ΔW: 0.164\n",
            "ITERATION: 329  | Loss 6.53  | ΔW: 0.163\n",
            "ITERATION: 330  | Loss 6.5  | ΔW: 0.164\n",
            "ITERATION: 331  | Loss 6.43  | ΔW: 0.176\n",
            "ITERATION: 332  | Loss 6.52  | ΔW: 0.169\n",
            "ITERATION: 333  | Loss 6.46  | ΔW: 0.166\n",
            "ITERATION: 334  | Loss 6.38  | ΔW: 0.171\n",
            "ITERATION: 335  | Loss 6.45  | ΔW: 0.168\n",
            "ITERATION: 336  | Loss 6.48  | ΔW: 0.166\n",
            "ITERATION: 337  | Loss 6.48  | ΔW: 0.171\n",
            "ITERATION: 338  | Loss 6.41  | ΔW: 0.183\n",
            "ITERATION: 339  | Loss 6.44  | ΔW: 0.18\n",
            "ITERATION: 340  | Loss 6.45  | ΔW: 0.175\n",
            "ITERATION: 341  | Loss 6.41  | ΔW: 0.199\n",
            "ITERATION: 342  | Loss 6.4  | ΔW: 0.178\n",
            "ITERATION: 343  | Loss 6.47  | ΔW: 0.181\n",
            "ITERATION: 344  | Loss 6.52  | ΔW: 0.179\n",
            "ITERATION: 345  | Loss 6.43  | ΔW: 0.177\n",
            "ITERATION: 346  | Loss 6.38  | ΔW: 0.181\n",
            "ITERATION: 347  | Loss 6.46  | ΔW: 0.18\n",
            "ITERATION: 348  | Loss 6.48  | ΔW: 0.183\n",
            "ITERATION: 349  | Loss 6.47  | ΔW: 0.188\n",
            "ITERATION: 350  | Loss 6.4  | ΔW: 0.175\n",
            "ITERATION: 351  | Loss 6.29  | ΔW: 0.19\n",
            "ITERATION: 352  | Loss 6.49  | ΔW: 0.177\n",
            "ITERATION: 353  | Loss 6.43  | ΔW: 0.191\n",
            "ITERATION: 354  | Loss 6.46  | ΔW: 0.192\n",
            "ITERATION: 355  | Loss 6.49  | ΔW: 0.192\n",
            "ITERATION: 356  | Loss 6.38  | ΔW: 0.192\n",
            "ITERATION: 357  | Loss 6.43  | ΔW: 0.186\n",
            "ITERATION: 358  | Loss 6.4  | ΔW: 0.193\n",
            "ITERATION: 359  | Loss 6.36  | ΔW: 0.191\n",
            "ITERATION: 360  | Loss 6.48  | ΔW: 0.199\n",
            "ITERATION: 361  | Loss 6.42  | ΔW: 0.201\n",
            "ITERATION: 362  | Loss 6.43  | ΔW: 0.192\n",
            "ITERATION: 363  | Loss 6.45  | ΔW: 0.209\n",
            "ITERATION: 364  | Loss 6.47  | ΔW: 0.19\n",
            "ITERATION: 365  | Loss 6.37  | ΔW: 0.192\n",
            "ITERATION: 366  | Loss 6.49  | ΔW: 0.205\n",
            "ITERATION: 367  | Loss 6.43  | ΔW: 0.186\n",
            "ITERATION: 368  | Loss 6.4  | ΔW: 0.202\n",
            "ITERATION: 369  | Loss 6.44  | ΔW: 0.216\n",
            "ITERATION: 370  | Loss 6.41  | ΔW: 0.205\n",
            "ITERATION: 371  | Loss 6.35  | ΔW: 0.208\n",
            "ITERATION: 372  | Loss 6.34  | ΔW: 0.21\n",
            "ITERATION: 373  | Loss 6.38  | ΔW: 0.221\n",
            "ITERATION: 374  | Loss 6.44  | ΔW: 0.227\n",
            "ITERATION: 375  | Loss 6.44  | ΔW: 0.209\n",
            "ITERATION: 376  | Loss 6.46  | ΔW: 0.233\n",
            "ITERATION: 377  | Loss 6.38  | ΔW: 0.219\n",
            "ITERATION: 378  | Loss 6.43  | ΔW: 0.235\n",
            "ITERATION: 379  | Loss 6.42  | ΔW: 0.203\n",
            "ITERATION: 380  | Loss 6.39  | ΔW: 0.251\n",
            "ITERATION: 381  | Loss 6.44  | ΔW: 0.212\n",
            "ITERATION: 382  | Loss 6.41  | ΔW: 0.242\n",
            "ITERATION: 383  | Loss 6.4  | ΔW: 0.216\n",
            "ITERATION: 384  | Loss 6.41  | ΔW: 0.211\n",
            "ITERATION: 385  | Loss 6.37  | ΔW: 0.245\n",
            "ITERATION: 386  | Loss 6.37  | ΔW: 0.227\n",
            "ITERATION: 387  | Loss 6.4  | ΔW: 0.23\n",
            "ITERATION: 388  | Loss 6.43  | ΔW: 0.232\n",
            "ITERATION: 389  | Loss 6.46  | ΔW: 0.25\n",
            "ITERATION: 390  | Loss 6.42  | ΔW: 0.233\n",
            "ITERATION: 391  | Loss 6.27  | ΔW: 0.227\n",
            "ITERATION: 392  | Loss 6.41  | ΔW: 0.246\n",
            "ITERATION: 393  | Loss 6.48  | ΔW: 0.237\n",
            "ITERATION: 394  | Loss 6.37  | ΔW: 0.254\n",
            "ITERATION: 395  | Loss 6.42  | ΔW: 0.262\n",
            "ITERATION: 396  | Loss 6.27  | ΔW: 0.235\n",
            "ITERATION: 397  | Loss 6.42  | ΔW: 0.233\n",
            "ITERATION: 398  | Loss 6.34  | ΔW: 0.242\n",
            "ITERATION: 399  | Loss 6.38  | ΔW: 0.253\n",
            "ITERATION: 400  | Loss 6.3  | ΔW: 0.268\n",
            "ITERATION: 401  | Loss 6.4  | ΔW: 0.259\n",
            "ITERATION: 402  | Loss 6.39  | ΔW: 0.278\n",
            "ITERATION: 403  | Loss 6.39  | ΔW: 0.249\n",
            "ITERATION: 404  | Loss 6.32  | ΔW: 0.262\n",
            "ITERATION: 405  | Loss 6.33  | ΔW: 0.26\n",
            "ITERATION: 406  | Loss 6.39  | ΔW: 0.246\n",
            "ITERATION: 407  | Loss 6.24  | ΔW: 0.26\n",
            "ITERATION: 408  | Loss 6.37  | ΔW: 0.272\n",
            "ITERATION: 409  | Loss 6.36  | ΔW: 0.271\n",
            "ITERATION: 410  | Loss 6.32  | ΔW: 0.257\n",
            "ITERATION: 411  | Loss 6.3  | ΔW: 0.25\n",
            "ITERATION: 412  | Loss 6.45  | ΔW: 0.245\n",
            "ITERATION: 413  | Loss 6.41  | ΔW: 0.268\n",
            "ITERATION: 414  | Loss 6.38  | ΔW: 0.254\n",
            "ITERATION: 415  | Loss 6.41  | ΔW: 0.246\n",
            "ITERATION: 416  | Loss 6.33  | ΔW: 0.269\n",
            "ITERATION: 417  | Loss 6.42  | ΔW: 0.302\n",
            "ITERATION: 418  | Loss 6.43  | ΔW: 0.263\n",
            "ITERATION: 419  | Loss 6.23  | ΔW: 0.258\n",
            "ITERATION: 420  | Loss 6.4  | ΔW: 0.309\n",
            "ITERATION: 421  | Loss 6.33  | ΔW: 0.271\n",
            "ITERATION: 422  | Loss 6.35  | ΔW: 0.274\n",
            "ITERATION: 423  | Loss 6.29  | ΔW: 0.276\n",
            "ITERATION: 424  | Loss 6.35  | ΔW: 0.284\n",
            "ITERATION: 425  | Loss 6.37  | ΔW: 0.282\n",
            "ITERATION: 426  | Loss 6.25  | ΔW: 0.297\n",
            "ITERATION: 427  | Loss 6.32  | ΔW: 0.311\n",
            "ITERATION: 428  | Loss 6.3  | ΔW: 0.279\n",
            "ITERATION: 429  | Loss 6.33  | ΔW: 0.279\n",
            "ITERATION: 430  | Loss 6.34  | ΔW: 0.282\n",
            "ITERATION: 431  | Loss 6.33  | ΔW: 0.273\n",
            "ITERATION: 432  | Loss 6.42  | ΔW: 0.289\n",
            "ITERATION: 433  | Loss 6.39  | ΔW: 0.292\n",
            "ITERATION: 434  | Loss 6.34  | ΔW: 0.295\n",
            "ITERATION: 435  | Loss 6.36  | ΔW: 0.293\n",
            "ITERATION: 436  | Loss 6.34  | ΔW: 0.281\n",
            "ITERATION: 437  | Loss 6.29  | ΔW: 0.299\n",
            "ITERATION: 438  | Loss 6.33  | ΔW: 0.298\n",
            "ITERATION: 439  | Loss 6.38  | ΔW: 0.306\n",
            "ITERATION: 440  | Loss 6.38  | ΔW: 0.308\n",
            "ITERATION: 441  | Loss 6.43  | ΔW: 0.318\n",
            "ITERATION: 442  | Loss 6.35  | ΔW: 0.336\n",
            "ITERATION: 443  | Loss 6.44  | ΔW: 0.31\n",
            "ITERATION: 444  | Loss 6.39  | ΔW: 0.321\n",
            "ITERATION: 445  | Loss 6.34  | ΔW: 0.318\n",
            "ITERATION: 446  | Loss 6.36  | ΔW: 0.315\n",
            "ITERATION: 447  | Loss 6.37  | ΔW: 0.343\n",
            "ITERATION: 448  | Loss 6.34  | ΔW: 0.347\n",
            "ITERATION: 449  | Loss 6.39  | ΔW: 0.336\n",
            "ITERATION: 450  | Loss 6.31  | ΔW: 0.328\n",
            "ITERATION: 451  | Loss 6.25  | ΔW: 0.413\n",
            "ITERATION: 452  | Loss 6.45  | ΔW: 0.333\n",
            "ITERATION: 453  | Loss 6.34  | ΔW: 0.333\n",
            "ITERATION: 454  | Loss 6.39  | ΔW: 0.33\n",
            "ITERATION: 455  | Loss 6.39  | ΔW: 0.351\n",
            "ITERATION: 456  | Loss 6.45  | ΔW: 0.349\n",
            "ITERATION: 457  | Loss 6.35  | ΔW: 0.353\n",
            "ITERATION: 458  | Loss 6.38  | ΔW: 0.331\n",
            "ITERATION: 459  | Loss 6.29  | ΔW: 0.344\n",
            "ITERATION: 460  | Loss 6.41  | ΔW: 0.357\n",
            "ITERATION: 461  | Loss 6.41  | ΔW: 0.353\n",
            "ITERATION: 462  | Loss 6.28  | ΔW: 0.349\n",
            "ITERATION: 463  | Loss 6.34  | ΔW: 0.37\n",
            "ITERATION: 464  | Loss 6.35  | ΔW: 0.378\n",
            "ITERATION: 465  | Loss 6.4  | ΔW: 0.339\n",
            "ITERATION: 466  | Loss 6.31  | ΔW: 0.348\n",
            "ITERATION: 467  | Loss 6.4  | ΔW: 0.36\n",
            "ITERATION: 468  | Loss 6.33  | ΔW: 0.37\n",
            "ITERATION: 469  | Loss 6.29  | ΔW: 0.405\n",
            "ITERATION: 470  | Loss 6.32  | ΔW: 0.402\n",
            "ITERATION: 471  | Loss 6.35  | ΔW: 0.375\n",
            "ITERATION: 472  | Loss 6.39  | ΔW: 0.388\n",
            "ITERATION: 473  | Loss 6.36  | ΔW: 0.377\n",
            "ITERATION: 474  | Loss 6.32  | ΔW: 0.423\n",
            "ITERATION: 475  | Loss 6.32  | ΔW: 0.4\n",
            "ITERATION: 476  | Loss 6.29  | ΔW: 0.418\n",
            "ITERATION: 477  | Loss 6.38  | ΔW: 0.377\n",
            "ITERATION: 478  | Loss 6.33  | ΔW: 0.405\n",
            "ITERATION: 479  | Loss 6.32  | ΔW: 0.399\n",
            "ITERATION: 480  | Loss 6.31  | ΔW: 0.394\n",
            "ITERATION: 481  | Loss 6.34  | ΔW: 0.406\n",
            "ITERATION: 482  | Loss 6.39  | ΔW: 0.411\n",
            "ITERATION: 483  | Loss 6.31  | ΔW: 0.413\n",
            "ITERATION: 484  | Loss 6.32  | ΔW: 0.435\n",
            "ITERATION: 485  | Loss 6.33  | ΔW: 0.414\n",
            "ITERATION: 486  | Loss 6.31  | ΔW: 0.414\n",
            "ITERATION: 487  | Loss 6.3  | ΔW: 0.413\n",
            "ITERATION: 488  | Loss 6.34  | ΔW: 0.436\n",
            "ITERATION: 489  | Loss 6.28  | ΔW: 0.425\n",
            "ITERATION: 490  | Loss 6.33  | ΔW: 0.427\n",
            "ITERATION: 491  | Loss 6.36  | ΔW: 0.437\n",
            "ITERATION: 492  | Loss 6.41  | ΔW: 0.449\n",
            "ITERATION: 493  | Loss 6.28  | ΔW: 0.475\n",
            "ITERATION: 494  | Loss 6.32  | ΔW: 0.436\n",
            "ITERATION: 495  | Loss 6.29  | ΔW: 0.449\n",
            "ITERATION: 496  | Loss 6.37  | ΔW: 0.445\n",
            "ITERATION: 497  | Loss 6.39  | ΔW: 0.444\n",
            "ITERATION: 498  | Loss 6.38  | ΔW: 0.454\n",
            "ITERATION: 499  | Loss 6.36  | ΔW: 0.434\n",
            "ITERATION: 500  | Loss 6.27  | ΔW: 0.455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformers_combined(\"GPT\",250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlJm-Q-gupCg",
        "outputId": "c9ba441d-cb5e-416b-bdc0-e1255e350a45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (37443 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:          1 | Train Loss 10.7307 | Validation Loss 10.7208\n",
            "Iteration:          2 | Train Loss 9.5013 | Validation Loss 9.5431\n",
            "Iteration:          3 | Train Loss 10.3556 | Validation Loss 10.5411\n",
            "Iteration:          4 | Train Loss 9.6130 | Validation Loss 9.7921\n",
            "Iteration:          5 | Train Loss 9.5026 | Validation Loss 9.7503\n",
            "Iteration:          6 | Train Loss 9.1903 | Validation Loss 9.5038\n",
            "Iteration:          7 | Train Loss 8.5632 | Validation Loss 8.9381\n",
            "Iteration:          8 | Train Loss 8.4724 | Validation Loss 8.8679\n",
            "Iteration:          9 | Train Loss 7.8931 | Validation Loss 8.3344\n",
            "Iteration:         10 | Train Loss 7.5371 | Validation Loss 8.2155\n",
            "Iteration:         11 | Train Loss 7.4141 | Validation Loss 8.0594\n",
            "Iteration:         12 | Train Loss 7.2912 | Validation Loss 7.8331\n",
            "Iteration:         13 | Train Loss 7.0710 | Validation Loss 7.7691\n",
            "Iteration:         14 | Train Loss 6.8637 | Validation Loss 7.5047\n",
            "Iteration:         15 | Train Loss 6.6007 | Validation Loss 7.3123\n",
            "Iteration:         16 | Train Loss 6.4860 | Validation Loss 7.1017\n",
            "Iteration:         17 | Train Loss 6.3251 | Validation Loss 6.9643\n",
            "Iteration:         18 | Train Loss 6.3350 | Validation Loss 7.0067\n",
            "Iteration:         19 | Train Loss 6.2294 | Validation Loss 6.9852\n",
            "Iteration:         20 | Train Loss 6.1243 | Validation Loss 6.8127\n",
            "Iteration:         21 | Train Loss 6.0639 | Validation Loss 6.8523\n",
            "Iteration:         22 | Train Loss 6.0586 | Validation Loss 6.8180\n",
            "Iteration:         23 | Train Loss 5.9853 | Validation Loss 6.7866\n",
            "Iteration:         24 | Train Loss 5.9596 | Validation Loss 6.7722\n",
            "Iteration:         25 | Train Loss 5.8425 | Validation Loss 6.7484\n",
            "Iteration:         26 | Train Loss 5.8779 | Validation Loss 6.7485\n",
            "Iteration:         27 | Train Loss 5.7603 | Validation Loss 6.6943\n",
            "Iteration:         28 | Train Loss 5.7483 | Validation Loss 6.6973\n",
            "Iteration:         29 | Train Loss 5.7411 | Validation Loss 6.6136\n",
            "Iteration:         30 | Train Loss 5.6809 | Validation Loss 6.7043\n",
            "Iteration:         31 | Train Loss 5.6464 | Validation Loss 6.6060\n",
            "Iteration:         32 | Train Loss 5.6324 | Validation Loss 6.5877\n",
            "Iteration:         33 | Train Loss 5.5502 | Validation Loss 6.5907\n",
            "Iteration:         34 | Train Loss 5.5381 | Validation Loss 6.5898\n",
            "Iteration:         35 | Train Loss 5.5554 | Validation Loss 6.4493\n",
            "Iteration:         36 | Train Loss 5.5074 | Validation Loss 6.5136\n",
            "Iteration:         37 | Train Loss 5.4550 | Validation Loss 6.4740\n",
            "Iteration:         38 | Train Loss 5.4620 | Validation Loss 6.4755\n",
            "Iteration:         39 | Train Loss 5.4007 | Validation Loss 6.4616\n",
            "Iteration:         40 | Train Loss 5.3931 | Validation Loss 6.4613\n",
            "Iteration:         41 | Train Loss 5.2273 | Validation Loss 6.3655\n",
            "Iteration:         42 | Train Loss 5.2359 | Validation Loss 6.4804\n",
            "Iteration:         43 | Train Loss 5.3021 | Validation Loss 6.5118\n",
            "Iteration:         44 | Train Loss 5.3152 | Validation Loss 6.4989\n",
            "Iteration:         45 | Train Loss 5.2284 | Validation Loss 6.3456\n",
            "Iteration:         46 | Train Loss 5.0637 | Validation Loss 6.3576\n",
            "Iteration:         47 | Train Loss 5.1728 | Validation Loss 6.3588\n",
            "Iteration:         48 | Train Loss 5.1352 | Validation Loss 6.4111\n",
            "Iteration:         49 | Train Loss 5.0955 | Validation Loss 6.3523\n",
            "Iteration:         50 | Train Loss 5.0680 | Validation Loss 6.3136\n",
            "Iteration:         51 | Train Loss 5.1039 | Validation Loss 6.4293\n",
            "Iteration:         52 | Train Loss 5.1397 | Validation Loss 6.2667\n",
            "Iteration:         53 | Train Loss 4.9910 | Validation Loss 6.3667\n",
            "Iteration:         54 | Train Loss 5.0064 | Validation Loss 6.2780\n",
            "Iteration:         55 | Train Loss 5.0531 | Validation Loss 6.2804\n",
            "Iteration:         56 | Train Loss 4.9216 | Validation Loss 6.3656\n",
            "Iteration:         57 | Train Loss 4.8816 | Validation Loss 6.2853\n",
            "Iteration:         58 | Train Loss 4.8878 | Validation Loss 6.2876\n",
            "Iteration:         59 | Train Loss 4.8754 | Validation Loss 6.3182\n",
            "Iteration:         60 | Train Loss 4.7735 | Validation Loss 6.2465\n",
            "Iteration:         61 | Train Loss 4.8333 | Validation Loss 6.2274\n",
            "Iteration:         62 | Train Loss 4.8190 | Validation Loss 6.2014\n",
            "Iteration:         63 | Train Loss 4.8129 | Validation Loss 6.2501\n",
            "Iteration:         64 | Train Loss 4.7701 | Validation Loss 6.2838\n",
            "Iteration:         65 | Train Loss 4.7291 | Validation Loss 6.2297\n",
            "Iteration:         66 | Train Loss 4.6786 | Validation Loss 6.2174\n",
            "Iteration:         67 | Train Loss 4.7225 | Validation Loss 6.1974\n",
            "Iteration:         68 | Train Loss 4.6314 | Validation Loss 6.2400\n",
            "Iteration:         69 | Train Loss 4.5794 | Validation Loss 6.2333\n",
            "Iteration:         70 | Train Loss 4.6602 | Validation Loss 6.1783\n",
            "Iteration:         71 | Train Loss 4.5355 | Validation Loss 6.1370\n",
            "Iteration:         72 | Train Loss 4.6391 | Validation Loss 6.1456\n",
            "Iteration:         73 | Train Loss 4.6191 | Validation Loss 6.2109\n",
            "Iteration:         74 | Train Loss 4.5799 | Validation Loss 6.2420\n",
            "Iteration:         75 | Train Loss 4.5294 | Validation Loss 6.2051\n",
            "Iteration:         76 | Train Loss 4.5443 | Validation Loss 6.2724\n",
            "Iteration:         77 | Train Loss 4.4018 | Validation Loss 6.1808\n",
            "Iteration:         78 | Train Loss 4.4583 | Validation Loss 6.2025\n",
            "Iteration:         79 | Train Loss 4.4244 | Validation Loss 6.2214\n",
            "Iteration:         80 | Train Loss 4.4230 | Validation Loss 6.2273\n",
            "Iteration:         81 | Train Loss 4.4291 | Validation Loss 6.1824\n",
            "Iteration:         82 | Train Loss 4.3661 | Validation Loss 6.1593\n",
            "Iteration:         83 | Train Loss 4.4440 | Validation Loss 6.1871\n",
            "Iteration:         84 | Train Loss 4.4291 | Validation Loss 6.2481\n",
            "Iteration:         85 | Train Loss 4.3812 | Validation Loss 6.1480\n",
            "Iteration:         86 | Train Loss 4.3373 | Validation Loss 6.1964\n",
            "Iteration:         87 | Train Loss 4.2981 | Validation Loss 6.1701\n",
            "Iteration:         88 | Train Loss 4.2600 | Validation Loss 6.2470\n",
            "Iteration:         89 | Train Loss 4.3468 | Validation Loss 6.2670\n",
            "Iteration:         90 | Train Loss 4.2741 | Validation Loss 6.1685\n",
            "Iteration:         91 | Train Loss 4.2320 | Validation Loss 6.2402\n",
            "Iteration:         92 | Train Loss 4.2537 | Validation Loss 6.1799\n",
            "Iteration:         93 | Train Loss 4.1993 | Validation Loss 6.2099\n",
            "Iteration:         94 | Train Loss 4.2736 | Validation Loss 6.1614\n",
            "Iteration:         95 | Train Loss 4.2127 | Validation Loss 6.1490\n",
            "Iteration:         96 | Train Loss 4.1582 | Validation Loss 6.1656\n",
            "Iteration:         97 | Train Loss 4.0377 | Validation Loss 6.0935\n",
            "Iteration:         98 | Train Loss 4.1644 | Validation Loss 6.1639\n",
            "Iteration:         99 | Train Loss 4.1090 | Validation Loss 6.2554\n",
            "Iteration:        100 | Train Loss 4.1930 | Validation Loss 6.1807\n",
            "Iteration:        101 | Train Loss 4.0702 | Validation Loss 6.2253\n",
            "Iteration:        102 | Train Loss 4.0895 | Validation Loss 6.2068\n",
            "Iteration:        103 | Train Loss 4.0412 | Validation Loss 6.1415\n",
            "Iteration:        104 | Train Loss 3.9899 | Validation Loss 6.1279\n",
            "Iteration:        105 | Train Loss 3.9705 | Validation Loss 6.2563\n",
            "Iteration:        106 | Train Loss 4.0208 | Validation Loss 6.2548\n",
            "Iteration:        107 | Train Loss 4.0237 | Validation Loss 6.1885\n",
            "Iteration:        108 | Train Loss 4.0098 | Validation Loss 6.2574\n",
            "Iteration:        109 | Train Loss 3.9625 | Validation Loss 6.2318\n",
            "Iteration:        110 | Train Loss 3.9690 | Validation Loss 6.1850\n",
            "Iteration:        111 | Train Loss 3.8801 | Validation Loss 6.1146\n",
            "Iteration:        112 | Train Loss 3.9554 | Validation Loss 6.2022\n",
            "Iteration:        113 | Train Loss 3.9024 | Validation Loss 6.2127\n",
            "Iteration:        114 | Train Loss 3.9477 | Validation Loss 6.1867\n",
            "Iteration:        115 | Train Loss 3.8517 | Validation Loss 6.1178\n",
            "Iteration:        116 | Train Loss 3.9209 | Validation Loss 6.1918\n",
            "Iteration:        117 | Train Loss 3.8539 | Validation Loss 6.1875\n",
            "Iteration:        118 | Train Loss 3.9082 | Validation Loss 6.1916\n",
            "Iteration:        119 | Train Loss 3.8057 | Validation Loss 6.2022\n",
            "Iteration:        120 | Train Loss 3.8517 | Validation Loss 6.1434\n",
            "Iteration:        121 | Train Loss 3.8410 | Validation Loss 6.1619\n",
            "Iteration:        122 | Train Loss 3.8412 | Validation Loss 6.1641\n",
            "Iteration:        123 | Train Loss 3.8532 | Validation Loss 6.1370\n",
            "Iteration:        124 | Train Loss 3.7863 | Validation Loss 6.1602\n",
            "Iteration:        125 | Train Loss 3.6809 | Validation Loss 6.2266\n",
            "Iteration:        126 | Train Loss 3.7693 | Validation Loss 6.1049\n",
            "Iteration:        127 | Train Loss 3.7193 | Validation Loss 6.1708\n",
            "Iteration:        128 | Train Loss 3.7093 | Validation Loss 6.2665\n",
            "Iteration:        129 | Train Loss 3.7896 | Validation Loss 6.1777\n",
            "Iteration:        130 | Train Loss 3.7764 | Validation Loss 6.1472\n",
            "Iteration:        131 | Train Loss 3.7814 | Validation Loss 6.2045\n",
            "Iteration:        132 | Train Loss 3.6881 | Validation Loss 6.2342\n",
            "Iteration:        133 | Train Loss 3.6962 | Validation Loss 6.1927\n",
            "Iteration:        134 | Train Loss 3.6375 | Validation Loss 6.1620\n",
            "Iteration:        135 | Train Loss 3.7206 | Validation Loss 6.2126\n",
            "Iteration:        136 | Train Loss 3.6487 | Validation Loss 6.2525\n",
            "Iteration:        137 | Train Loss 3.6201 | Validation Loss 6.3392\n",
            "Iteration:        138 | Train Loss 3.6115 | Validation Loss 6.2740\n",
            "Iteration:        139 | Train Loss 3.6320 | Validation Loss 6.1921\n",
            "Iteration:        140 | Train Loss 3.5787 | Validation Loss 6.3082\n",
            "Iteration:        141 | Train Loss 3.6200 | Validation Loss 6.3888\n",
            "Iteration:        142 | Train Loss 3.6459 | Validation Loss 6.3295\n",
            "Iteration:        143 | Train Loss 3.5430 | Validation Loss 6.2046\n",
            "Iteration:        144 | Train Loss 3.4676 | Validation Loss 6.1800\n",
            "Iteration:        145 | Train Loss 3.5794 | Validation Loss 6.2550\n",
            "Iteration:        146 | Train Loss 3.4613 | Validation Loss 6.2152\n",
            "Iteration:        147 | Train Loss 3.4983 | Validation Loss 6.1668\n",
            "Iteration:        148 | Train Loss 3.5134 | Validation Loss 6.2373\n",
            "Iteration:        149 | Train Loss 3.4526 | Validation Loss 6.2148\n",
            "Iteration:        150 | Train Loss 3.4505 | Validation Loss 6.2137\n",
            "Iteration:        151 | Train Loss 3.4789 | Validation Loss 6.2485\n",
            "Iteration:        152 | Train Loss 3.4645 | Validation Loss 6.1074\n",
            "Iteration:        153 | Train Loss 3.4720 | Validation Loss 6.3129\n",
            "Iteration:        154 | Train Loss 3.4892 | Validation Loss 6.3070\n",
            "Iteration:        155 | Train Loss 3.3558 | Validation Loss 6.2926\n",
            "Iteration:        156 | Train Loss 3.4151 | Validation Loss 6.2569\n",
            "Iteration:        157 | Train Loss 3.4000 | Validation Loss 6.3073\n",
            "Iteration:        158 | Train Loss 3.3486 | Validation Loss 6.2919\n",
            "Iteration:        159 | Train Loss 3.3422 | Validation Loss 6.2437\n",
            "Iteration:        160 | Train Loss 3.3662 | Validation Loss 6.2470\n",
            "Iteration:        161 | Train Loss 3.3780 | Validation Loss 6.2647\n",
            "Iteration:        162 | Train Loss 3.4057 | Validation Loss 6.3304\n",
            "Iteration:        163 | Train Loss 3.3578 | Validation Loss 6.3758\n",
            "Iteration:        164 | Train Loss 3.2805 | Validation Loss 6.3085\n",
            "Iteration:        165 | Train Loss 3.2437 | Validation Loss 6.3630\n",
            "Iteration:        166 | Train Loss 3.3245 | Validation Loss 6.3271\n",
            "Iteration:        167 | Train Loss 3.2244 | Validation Loss 6.2958\n",
            "Iteration:        168 | Train Loss 3.2719 | Validation Loss 6.3917\n",
            "Iteration:        169 | Train Loss 3.2219 | Validation Loss 6.3086\n",
            "Iteration:        170 | Train Loss 3.2680 | Validation Loss 6.2971\n",
            "Iteration:        171 | Train Loss 3.2344 | Validation Loss 6.4218\n",
            "Iteration:        172 | Train Loss 3.2228 | Validation Loss 6.3094\n",
            "Iteration:        173 | Train Loss 3.2755 | Validation Loss 6.3578\n",
            "Iteration:        174 | Train Loss 3.2135 | Validation Loss 6.2826\n",
            "Iteration:        175 | Train Loss 3.1758 | Validation Loss 6.3510\n",
            "Iteration:        176 | Train Loss 3.2245 | Validation Loss 6.3083\n",
            "Iteration:        177 | Train Loss 3.2387 | Validation Loss 6.3560\n",
            "Iteration:        178 | Train Loss 3.2255 | Validation Loss 6.3117\n",
            "Iteration:        179 | Train Loss 3.1654 | Validation Loss 6.2603\n",
            "Iteration:        180 | Train Loss 3.1788 | Validation Loss 6.3436\n",
            "Iteration:        181 | Train Loss 3.1555 | Validation Loss 6.4482\n",
            "Iteration:        182 | Train Loss 3.1034 | Validation Loss 6.2358\n",
            "Iteration:        183 | Train Loss 3.1856 | Validation Loss 6.3348\n",
            "Iteration:        184 | Train Loss 3.1535 | Validation Loss 6.4643\n",
            "Iteration:        185 | Train Loss 3.1204 | Validation Loss 6.3041\n",
            "Iteration:        186 | Train Loss 3.0918 | Validation Loss 6.2420\n",
            "Iteration:        187 | Train Loss 3.1852 | Validation Loss 6.3479\n",
            "Iteration:        188 | Train Loss 3.1091 | Validation Loss 6.3253\n",
            "Iteration:        189 | Train Loss 3.0549 | Validation Loss 6.3253\n",
            "Iteration:        190 | Train Loss 3.1014 | Validation Loss 6.3253\n",
            "Iteration:        191 | Train Loss 3.0546 | Validation Loss 6.4714\n",
            "Iteration:        192 | Train Loss 3.0979 | Validation Loss 6.4607\n",
            "Iteration:        193 | Train Loss 3.0123 | Validation Loss 6.4236\n",
            "Iteration:        194 | Train Loss 3.0887 | Validation Loss 6.3251\n",
            "Iteration:        195 | Train Loss 3.0801 | Validation Loss 6.3212\n",
            "Iteration:        196 | Train Loss 3.0258 | Validation Loss 6.3620\n",
            "Iteration:        197 | Train Loss 2.9947 | Validation Loss 6.2805\n",
            "Iteration:        198 | Train Loss 3.0003 | Validation Loss 6.3908\n",
            "Iteration:        199 | Train Loss 3.0767 | Validation Loss 6.3822\n",
            "Iteration:        200 | Train Loss 3.0203 | Validation Loss 6.3726\n",
            "Iteration:        201 | Train Loss 2.9173 | Validation Loss 6.3630\n",
            "Iteration:        202 | Train Loss 2.9145 | Validation Loss 6.4671\n",
            "Iteration:        203 | Train Loss 2.9606 | Validation Loss 6.4172\n",
            "Iteration:        204 | Train Loss 2.9709 | Validation Loss 6.4183\n",
            "Iteration:        205 | Train Loss 2.9272 | Validation Loss 6.4857\n",
            "Iteration:        206 | Train Loss 2.9992 | Validation Loss 6.4680\n",
            "Iteration:        207 | Train Loss 2.9984 | Validation Loss 6.4959\n",
            "Iteration:        208 | Train Loss 2.9107 | Validation Loss 6.6739\n",
            "Iteration:        209 | Train Loss 2.8350 | Validation Loss 6.4895\n",
            "Iteration:        210 | Train Loss 2.8457 | Validation Loss 6.3396\n",
            "Iteration:        211 | Train Loss 2.9249 | Validation Loss 6.4212\n",
            "Iteration:        212 | Train Loss 2.8849 | Validation Loss 6.5468\n",
            "Iteration:        213 | Train Loss 2.8049 | Validation Loss 6.3226\n",
            "Iteration:        214 | Train Loss 2.7973 | Validation Loss 6.3384\n",
            "Iteration:        215 | Train Loss 2.8980 | Validation Loss 6.3937\n",
            "Iteration:        216 | Train Loss 2.7649 | Validation Loss 6.3913\n",
            "Iteration:        217 | Train Loss 2.7962 | Validation Loss 6.3966\n",
            "Iteration:        218 | Train Loss 2.7987 | Validation Loss 6.2833\n",
            "Iteration:        219 | Train Loss 2.7219 | Validation Loss 6.3492\n",
            "Iteration:        220 | Train Loss 2.8356 | Validation Loss 6.4653\n",
            "Iteration:        221 | Train Loss 2.8528 | Validation Loss 6.3568\n",
            "Iteration:        222 | Train Loss 2.7581 | Validation Loss 6.4457\n",
            "Iteration:        223 | Train Loss 2.6867 | Validation Loss 6.5852\n",
            "Iteration:        224 | Train Loss 2.7366 | Validation Loss 6.4645\n",
            "Iteration:        225 | Train Loss 2.7490 | Validation Loss 6.5088\n",
            "Iteration:        226 | Train Loss 2.6869 | Validation Loss 6.5132\n",
            "Iteration:        227 | Train Loss 2.6636 | Validation Loss 6.4791\n",
            "Iteration:        228 | Train Loss 2.6939 | Validation Loss 6.5453\n",
            "Iteration:        229 | Train Loss 2.7103 | Validation Loss 6.5107\n",
            "Iteration:        230 | Train Loss 2.7008 | Validation Loss 6.4825\n",
            "Iteration:        231 | Train Loss 2.6480 | Validation Loss 6.3979\n",
            "Iteration:        232 | Train Loss 2.7001 | Validation Loss 6.6115\n",
            "Iteration:        233 | Train Loss 2.6364 | Validation Loss 6.4800\n",
            "Iteration:        234 | Train Loss 2.6516 | Validation Loss 6.5324\n",
            "Iteration:        235 | Train Loss 2.6101 | Validation Loss 6.5856\n",
            "Iteration:        236 | Train Loss 2.5883 | Validation Loss 6.5621\n",
            "Iteration:        237 | Train Loss 2.5884 | Validation Loss 6.3255\n",
            "Iteration:        238 | Train Loss 2.6290 | Validation Loss 6.5534\n",
            "Iteration:        239 | Train Loss 2.6220 | Validation Loss 6.6908\n",
            "Iteration:        240 | Train Loss 2.6259 | Validation Loss 6.5190\n",
            "Iteration:        241 | Train Loss 2.6332 | Validation Loss 6.4944\n",
            "Iteration:        242 | Train Loss 2.5699 | Validation Loss 6.5831\n",
            "Iteration:        243 | Train Loss 2.5515 | Validation Loss 6.5357\n",
            "Iteration:        244 | Train Loss 2.5824 | Validation Loss 6.6013\n",
            "Iteration:        245 | Train Loss 2.5224 | Validation Loss 6.4881\n",
            "Iteration:        246 | Train Loss 2.5619 | Validation Loss 6.6376\n",
            "Iteration:        247 | Train Loss 2.5360 | Validation Loss 6.4721\n",
            "Iteration:        248 | Train Loss 2.4306 | Validation Loss 6.5773\n",
            "Iteration:        249 | Train Loss 2.5353 | Validation Loss 6.5059\n",
            "Iteration:        250 | Train Loss 2.4792 | Validation Loss 6.5251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformers_combined(\"VIT\",50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917,
          "referenced_widgets": [
            "0a72e33b1b6442daba3eb9c59477aeea",
            "6ea5c43c605c49d5b7e463673e78dedc",
            "d101529ab25345bf816924d8feaad369",
            "c7a59caf15b44fb3ba8c499140ff37e4",
            "6f7d130a0a894d51a82e37d9456e030d",
            "c2ebaa76c6d84b79929c51db29736680",
            "320e4ea320d14bd8a7218f2d960cdd78",
            "fa7e73c7aa1e40e08aafa879ae760755",
            "73921466763f4f3c89bd241ba2614700",
            "0df377f22da14914b24cea651f670bc4",
            "30349fd000be4ef2a972b2e1ec59ae0e"
          ]
        },
        "id": "cZbUDdqBmTRy",
        "outputId": "df0f96f8-dffd-431e-9930-cf53e0f293b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a72e33b1b6442daba3eb9c59477aeea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 4.8212 | train_acc: 0.2383 | test_loss: 1.0424 | test_acc: 0.5417\n",
            "Epoch: 2 | train_loss: 1.2359 | train_acc: 0.4453 | test_loss: 3.0713 | test_acc: 0.1979\n",
            "Epoch: 3 | train_loss: 1.4682 | train_acc: 0.2578 | test_loss: 1.0930 | test_acc: 0.1979\n",
            "Epoch: 4 | train_loss: 1.1189 | train_acc: 0.3086 | test_loss: 1.3429 | test_acc: 0.2604\n",
            "Epoch: 5 | train_loss: 1.2215 | train_acc: 0.2969 | test_loss: 1.0885 | test_acc: 0.5417\n",
            "Epoch: 6 | train_loss: 1.1224 | train_acc: 0.2969 | test_loss: 1.3156 | test_acc: 0.1979\n",
            "Epoch: 7 | train_loss: 1.2040 | train_acc: 0.3008 | test_loss: 1.1932 | test_acc: 0.2604\n",
            "Epoch: 8 | train_loss: 1.3324 | train_acc: 0.3086 | test_loss: 1.2780 | test_acc: 0.1979\n",
            "Epoch: 9 | train_loss: 1.3068 | train_acc: 0.2930 | test_loss: 1.1791 | test_acc: 0.2604\n",
            "Epoch: 10 | train_loss: 1.2368 | train_acc: 0.4297 | test_loss: 1.0127 | test_acc: 0.5417\n",
            "Epoch: 11 | train_loss: 1.2831 | train_acc: 0.2695 | test_loss: 1.1493 | test_acc: 0.1979\n",
            "Epoch: 12 | train_loss: 1.1971 | train_acc: 0.2969 | test_loss: 1.1360 | test_acc: 0.2604\n",
            "Epoch: 13 | train_loss: 1.1962 | train_acc: 0.2930 | test_loss: 1.1615 | test_acc: 0.2604\n",
            "Epoch: 14 | train_loss: 1.2461 | train_acc: 0.2969 | test_loss: 1.0811 | test_acc: 0.2604\n",
            "Epoch: 15 | train_loss: 1.2190 | train_acc: 0.3203 | test_loss: 1.2172 | test_acc: 0.1979\n",
            "Epoch: 16 | train_loss: 1.2264 | train_acc: 0.2852 | test_loss: 1.0366 | test_acc: 0.5417\n",
            "Epoch: 17 | train_loss: 1.2814 | train_acc: 0.2930 | test_loss: 1.2488 | test_acc: 0.1979\n",
            "Epoch: 18 | train_loss: 1.1172 | train_acc: 0.4062 | test_loss: 1.2783 | test_acc: 0.2604\n",
            "Epoch: 19 | train_loss: 1.1688 | train_acc: 0.2656 | test_loss: 1.1266 | test_acc: 0.1979\n",
            "Epoch: 20 | train_loss: 1.2134 | train_acc: 0.2930 | test_loss: 1.0960 | test_acc: 0.1979\n",
            "Epoch: 21 | train_loss: 1.1283 | train_acc: 0.3945 | test_loss: 1.0194 | test_acc: 0.5417\n",
            "Epoch: 22 | train_loss: 1.1232 | train_acc: 0.4023 | test_loss: 1.2094 | test_acc: 0.1979\n",
            "Epoch: 23 | train_loss: 1.1964 | train_acc: 0.2930 | test_loss: 1.0920 | test_acc: 0.1979\n",
            "Epoch: 24 | train_loss: 1.2524 | train_acc: 0.2617 | test_loss: 1.0549 | test_acc: 0.5417\n",
            "Epoch: 25 | train_loss: 1.1304 | train_acc: 0.3047 | test_loss: 1.3959 | test_acc: 0.1979\n",
            "Epoch: 26 | train_loss: 1.1160 | train_acc: 0.3945 | test_loss: 1.1323 | test_acc: 0.2604\n",
            "Epoch: 27 | train_loss: 1.1474 | train_acc: 0.3047 | test_loss: 1.1028 | test_acc: 0.2604\n",
            "Epoch: 28 | train_loss: 1.1495 | train_acc: 0.2969 | test_loss: 1.1626 | test_acc: 0.1979\n",
            "Epoch: 29 | train_loss: 1.0972 | train_acc: 0.4258 | test_loss: 1.0218 | test_acc: 0.5417\n",
            "Epoch: 30 | train_loss: 1.1661 | train_acc: 0.2812 | test_loss: 1.0615 | test_acc: 0.5417\n",
            "Epoch: 31 | train_loss: 1.1245 | train_acc: 0.2930 | test_loss: 1.3248 | test_acc: 0.2604\n",
            "Epoch: 32 | train_loss: 1.1022 | train_acc: 0.4336 | test_loss: 1.2106 | test_acc: 0.1979\n",
            "Epoch: 33 | train_loss: 1.1486 | train_acc: 0.2930 | test_loss: 1.1365 | test_acc: 0.1979\n",
            "Epoch: 34 | train_loss: 1.1337 | train_acc: 0.2773 | test_loss: 1.0962 | test_acc: 0.2604\n",
            "Epoch: 35 | train_loss: 1.1472 | train_acc: 0.3164 | test_loss: 1.0449 | test_acc: 0.5417\n",
            "Epoch: 36 | train_loss: 1.1387 | train_acc: 0.2930 | test_loss: 1.1721 | test_acc: 0.1979\n",
            "Epoch: 37 | train_loss: 1.1098 | train_acc: 0.3008 | test_loss: 1.0785 | test_acc: 0.5417\n",
            "Epoch: 38 | train_loss: 1.0921 | train_acc: 0.4219 | test_loss: 1.1427 | test_acc: 0.1979\n",
            "Epoch: 39 | train_loss: 1.1070 | train_acc: 0.2930 | test_loss: 1.2060 | test_acc: 0.1979\n",
            "Epoch: 40 | train_loss: 1.1235 | train_acc: 0.2930 | test_loss: 1.1702 | test_acc: 0.2604\n",
            "Epoch: 41 | train_loss: 1.0915 | train_acc: 0.4375 | test_loss: 1.1651 | test_acc: 0.1979\n",
            "Epoch: 42 | train_loss: 1.1275 | train_acc: 0.2930 | test_loss: 1.1333 | test_acc: 0.1979\n",
            "Epoch: 43 | train_loss: 1.1337 | train_acc: 0.2969 | test_loss: 1.0577 | test_acc: 0.5417\n",
            "Epoch: 44 | train_loss: 1.0832 | train_acc: 0.4062 | test_loss: 1.1199 | test_acc: 0.2604\n",
            "Epoch: 45 | train_loss: 1.1400 | train_acc: 0.3047 | test_loss: 1.1687 | test_acc: 0.2604\n",
            "Epoch: 46 | train_loss: 1.1354 | train_acc: 0.3047 | test_loss: 1.1585 | test_acc: 0.1979\n",
            "Epoch: 47 | train_loss: 1.1113 | train_acc: 0.2969 | test_loss: 1.0857 | test_acc: 0.5417\n",
            "Epoch: 48 | train_loss: 1.1107 | train_acc: 0.2773 | test_loss: 1.1000 | test_acc: 0.2604\n",
            "Epoch: 49 | train_loss: 1.0927 | train_acc: 0.3906 | test_loss: 1.1426 | test_acc: 0.1979\n",
            "Epoch: 50 | train_loss: 1.1131 | train_acc: 0.2930 | test_loss: 1.1673 | test_acc: 0.1979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1CqYMZizwfnR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}